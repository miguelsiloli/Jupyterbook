{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Improving the efficiency of HTML parsers\n",
    "\n",
    "The context of this post is to explore and find a solution to the costly text parsing of html files. This is crucial when developing a webscraper. There are arguably decent solutions such as the Scrapy infrastructure which can parse nodes and data based on their own custom dataclasses. This solution is interesting as it provides an incremental approach (the bulk parsing is done sequentially for each html page parsed), however from a POV of Data Engineering, the good practice should be to parse the raw data as a whole to assure consistency, availability and the ability to do some backfilling if the html content changes.\n",
    "\n",
    "For this analysis we are using **line_profiler** package.\n",
    "\n",
    "**Keywords**: webscraping, html-parsing, beautifulsoup4, dask, lxml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The html parsing of a daily batch of files (about 300 html at about 40mb size total) was taking over 8 minutes.\n",
    "Our problem consists in parsing hundreds of raw html files with a similar structure, retrieve the data in a structured format and save it to parquet.\n",
    "\n",
    "We are going to explore the following methods:\n",
    "- Pandas + BS4 (baseline)\n",
    "  - This is my first approach to any problem of this kind. Make it work with the least resources spent. I use Pandas and BeautifulSoup4 because their are pretty much the simplest packages for their niche.\n",
    "- Dask + BS4\n",
    "  - My first optimization was to introduce Dask instead of Pandas due to their similar semantics. However, it raised more issues than I would expect when the html files were invalid.\n",
    "- Dask + BS4 with optimizations\n",
    "  - Now I started to work on BS4 optimizations by deconstructing a unique parsing function into multiple functions.\n",
    "- Dask + lxml\n",
    "  - Finally I wasn't happy with the results of BS4\n",
    "\n",
    "### Process\n",
    "\n",
    "```{mermaid}\n",
    "sequenceDiagram\n",
    "    participant HTMLFiles as HTML Files<br>(Local or Cloud)\n",
    "    participant Parser Script\n",
    "    participant ChunkReader as HTML File Chunk<br>Reader\n",
    "    participant DaskTasks as Dask Delayed Tasks<br>(Parallel Processing)\n",
    "    participant Database\n",
    "\n",
    "    HTMLFiles ->> Parser Script: Batched HTML Files<br>Processed in Chunks\n",
    "    Parser Script ->> ChunkReader: Read HTML File Chunks\n",
    "    ChunkReader -->> Parser Script: HTML File Data\n",
    "    Parser Script ->> DaskTasks: Processed and Merged DataFrames\n",
    "    DaskTasks -->> Parser Script: DataFrames\n",
    "    Parser Script ->> Database: SQL Commands<br>Bulk Insert\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "\n",
    "```{mermaid}\n",
    "erDiagram\n",
    "    LISTINGS {\n",
    "        string title\n",
    "        string home_type\n",
    "        string link\n",
    "        string garage\n",
    "        int price\n",
    "        string additional_details\n",
    "        string description\n",
    "        string home_size\n",
    "        int home_area\n",
    "        int floor\n",
    "        boolean elevator\n",
    "        float price_per_sqr_meter\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'duck_handler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mline_profiler\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mduck_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatabaseConnection\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProgressBar\n\u001b[0;32m     11\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mline_profiler\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'duck_handler'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "import os\n",
    "import duckdb\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import line_profiler\n",
    "from duck_handler import DatabaseConnection\n",
    "from dask.diagnostics import ProgressBar\n",
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas with BS4 parsing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_to_dataframe(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        listings = []\n",
    "\n",
    "        # Extract each article which represents a real estate listing\n",
    "        articles = soup.find_all('article', class_=['item', 'extended-item'])\n",
    "        for article in articles:\n",
    "            # Initialize dictionary to store listing details\n",
    "            listing_info = {}\n",
    "            \n",
    "            # Extracting the title and link of the property\n",
    "            title_link = article.find('a', class_='item-link')\n",
    "            if title_link:\n",
    "                listing_info['title'] = title_link.get('title', '')\n",
    "                # Split the title by space and take the first word as home_type\n",
    "                title_words = listing_info['title'].split()\n",
    "                if title_words:  # Check if there is at least one word\n",
    "                    listing_info['home_type'] = title_words[0]\n",
    "                else:\n",
    "                    listing_info['home_type'] = ''  # Handle cases where the title might be empty\n",
    "                listing_info['link'] = 'https://www.idealista.pt' + title_link.get('href', '')\n",
    "\n",
    "            # Extracting price information\n",
    "            price_info = article.find('span', class_='item-price')\n",
    "            parking_info = article.find('span', class_='item-parking')\n",
    "            if parking_info:\n",
    "                listing_info['garage'] = \"Yes\"\n",
    "            else:\n",
    "                listing_info['garage'] = \"No\"\n",
    "            if price_info:\n",
    "                numeric_price = re.findall(r'\\d+\\.\\d+|\\d+', price_info.text.strip())\n",
    "                if numeric_price:\n",
    "                    listing_info['price'] = numeric_price[0]  \n",
    "\n",
    "            # Extracting detail characters like 'T1', '55 m² área bruta', etc.\n",
    "            details = article.find_all('span', class_='item-detail')\n",
    "            details_text = [detail.get_text(strip=True) for detail in details]\n",
    "            listing_info[\"additional_details\"] = details_text\n",
    "            description = article.find('div', class_=\"item-description description\").text.strip()\n",
    "            listing_info['description'] = description\n",
    "\n",
    "            # Adding the listing to the list\n",
    "            listings.append(listing_info)\n",
    "\n",
    "        # Creating a DataFrame\n",
    "        df = pd.DataFrame(listings)\n",
    "\n",
    "        # Initialize the new columns\n",
    "        df['home_size'] = None\n",
    "        df['home_area'] = None\n",
    "        df['floor'] = None\n",
    "\n",
    "        # Iterate over rows and fill the columns accordingly\n",
    "        for idx, row in df.iterrows():\n",
    "            details = row['additional_details']\n",
    "            if len(details) >= 2:\n",
    "                df.at[idx, 'home_size'] = details[0]\n",
    "                df.at[idx, 'home_area'] = int(re.search(r'\\d+', details[1]).group())\n",
    "                if len(details) >= 3 and 'andar' in details[2]:\n",
    "                    df.at[idx, 'floor'] = details[2]\n",
    "\n",
    "        # Drop the original 'additional_details' column\n",
    "        # df.drop(columns=['additional_details'], inplace=True)\n",
    "        df['elevator'] = df['floor'].str.contains('com elevador')\n",
    "        df['floor'] = df['floor'].apply(lambda x: int(re.search(r'\\d+', str(x)).group()) if x is not None else None)\n",
    "        df['price'] = df['price'].str.replace('.', '')\n",
    "        df['price'] = df['price'].astype(int)\n",
    "        df['price_per_sqr_meter'] = df['price']/df['home_area']\n",
    "        df['elevator'] = df['elevator'].fillna(False)\n",
    "        df['floor'] = df['floor'].fillna(0)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_pandas():\n",
    "    directory_path = \"raw/idealista\"\n",
    "\n",
    "    # List HTML files in the directory\n",
    "    html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "\n",
    "    # Parse HTML files and create a Dask DataFrame\n",
    "    data = [parse_html_to_dataframe(file_path) for file_path in html_files]\n",
    "    df = pd.concat(data, axis = 0, ignore_index= True)\n",
    "\n",
    "    # Connect to DuckDB\n",
    "    con = duckdb.connect(database='house_prices.db', read_only=False)\n",
    "\n",
    "    # Query to create a DuckDB table from DataFrame\n",
    "    create_table_query = f\"CREATE TABLE html_data AS SELECT * FROM df\"\n",
    "\n",
    "    # Execute the query\n",
    "    con.execute(\"DROP TABLE IF EXISTS html_data\")\n",
    "    con.execute(create_table_query)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%lprun` not found.\n"
     ]
    }
   ],
   "source": [
    "%lprun -f process_data_pandas process_data_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Total time: 478.326 s\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def process_data_pandas():\n",
    "     2         1          4.0      4.0      0.0      directory_path = \"idealista\"\n",
    "     3                                           \n",
    "     4                                               # List HTML files in the directory\n",
    "     5         1      40263.0  40263.0      0.1      html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "     6                                           \n",
    "     7                                               # Parse HTML files and create a Dask DataFrame\n",
    "     8         1   47792357.0    5e+07     99.9      data = [parse_html_to_dataframe(file_path) for file_path in html_files]\n",
    "     9                                               df = pd.concat(data, axis = 0, ignore_index= True)\n",
    "    10                                           \n",
    "    11                                               # Connect to DuckDB\n",
    "    12                                               con = duckdb.connect(database='house_prices.sql', read_only=False)\n",
    "    13                                           \n",
    "    14                                               # Query to create a DuckDB table from DataFrame\n",
    "    15                                               create_table_query = f\"CREATE TABLE html_data AS SELECT * FROM df\"\n",
    "    16                                           \n",
    "    17                                               # Execute the query\n",
    "    18                                               con.execute(\"DROP TABLE IF EXISTS html_data\")\n",
    "    19                                               con.execute(create_table_query)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Dask with BS4 parsing function](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(parsing_function, directory_path=\"idealista\"):\n",
    "    # Check connection first\n",
    "    db = DatabaseConnection('house_prices.db')\n",
    "    with db.managed_cursor() as con:\n",
    "        # List HTML files in the directory\n",
    "        html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "        \n",
    "        # Use Dask to parse HTML files asynchronously\n",
    "        delayed_dataframes = [dask.delayed(parsing_function)(file) for file in html_files]\n",
    "        with ProgressBar():\n",
    "            # i cant maintain lazy eval because there are some empty dataframes due to parsing inconsistencies\n",
    "            computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
    "\n",
    "        # Filter out empty dataframes\n",
    "        filtered_dataframes = [df for df in computed_dataframes if df is not None and not df.empty]\n",
    "\n",
    "        # Concatenate dataframes efficiently\n",
    "        if filtered_dataframes:\n",
    "            pandas_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "            # Insert into DuckDB\n",
    "            con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "            pandas_df.to_sql(directory_path, con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's actually an issue which hinders performance which has to do with the html file structure. There are some files which were parsed and didn't contain data as the parsing function expected, so I had to compute the list of dataframes before actually merging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 16.59 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1858589816.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pandas_df.to_sql(directory_path, con, index=False, if_exists='replace')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 257.473 s\n",
      "File: C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1858589816.py\n",
      "Function: process_data at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def process_data(parsing_function, directory_path=\"idealista\"):\n",
      "     2                                               # Check connection first\n",
      "     3         1         33.0     33.0      0.0      db = DatabaseConnection('house_prices.db')\n",
      "     4         2    2430868.0    1e+06      0.1      with db.managed_cursor() as con:\n",
      "     5                                                   # List HTML files in the directory\n",
      "     6         1      44211.0  44211.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
      "     7                                                   \n",
      "     8                                                   # Use Dask to parse HTML files asynchronously\n",
      "     9         1     320066.0 320066.0      0.0          delayed_dataframes = [dask.delayed(parsing_function)(file) for file in html_files]\n",
      "    10         2        394.0    197.0      0.0          with ProgressBar():\n",
      "    11                                                       # i cant maintain lazy eval because there are some empty dataframes due to parsing inconsistencies\n",
      "    12         1  169339803.0    2e+08      6.6              computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
      "    13                                           \n",
      "    14                                                   # Filter out empty dataframes\n",
      "    15         1      27943.0  27943.0      0.0          filtered_dataframes = [df for df in computed_dataframes if df is not None and not df.empty]\n",
      "    16                                           \n",
      "    17                                                   # Concatenate dataframes efficiently\n",
      "    18         1          7.0      7.0      0.0          if filtered_dataframes:\n",
      "    19         1     616761.0 616761.0      0.0              pandas_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
      "    20                                           \n",
      "    21                                                       # Insert into DuckDB\n",
      "    22         1     356003.0 356003.0      0.0              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
      "    23         1 2401595989.0    2e+09     93.3              pandas_df.to_sql(directory_path, con, index=False, if_exists='replace')"
     ]
    }
   ],
   "source": [
    "%lprun -f process_data process_data(parse_html_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timer unit: 1e-07 s\n",
    "\n",
    "Total time: 257.473 s\n",
    "```\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def process_data(parsing_function, directory_path=\"idealista\"):\n",
    "     2                                               # Check connection first\n",
    "     3         1         33.0     33.0      0.0      db = DatabaseConnection('house_prices.db')\n",
    "     4         2    2430868.0    1e+06      0.1      with db.managed_cursor() as con:\n",
    "     5                                                   # List HTML files in the directory\n",
    "     6         1      44211.0  44211.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "     7                                                   \n",
    "     8                                                   # Use Dask to parse HTML files asynchronously\n",
    "     9         1     320066.0 320066.0      0.0          delayed_dataframes = [dask.delayed(parsing_function)(file) for file in html_files]\n",
    "    10         2        394.0    197.0      0.0          with ProgressBar():\n",
    "    11                                                       # i cant maintain lazy eval because there are some empty dataframes due to parsing inconsistencies\n",
    "    12         1  169339803.0    2e+08      6.6              computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
    "    13                                           \n",
    "    14                                                   # Filter out empty dataframes\n",
    "    15         1      27943.0  27943.0      0.0          filtered_dataframes = [df for df in computed_dataframes if df is not None and not df.empty]\n",
    "    16                                           \n",
    "    17                                                   # Concatenate dataframes efficiently\n",
    "...\n",
    "    19         1     616761.0 616761.0      0.0              pandas_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "    20                                           \n",
    "    21                                                       # Insert into DuckDB\n",
    "    22         1     356003.0 356003.0      0.0              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "    23         1 2401595989.0    2e+09     93.3              pandas_df.to_sql(directory_path, con, index=False, if_exists='replace')```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Dask with BS4 with some improvements](#toc0_)\n",
    "\n",
    "**Function Decomposition**: Break down the process_data function into smaller, more manageable functions. This can improve readability and potentially uncover more opportunities for optimization.\n",
    "\n",
    "\n",
    "**Adjusting the chunk sizes** in Dask processing can indeed be beneficial if your HTML files are relatively small. By batching these files into larger tasks, you reduce the overhead that comes from managing many small tasks.\n",
    "\n",
    "\n",
    "Steps to Implement Batching:\n",
    "\n",
    "- Group HTML Files: Divide the list of HTML files into batches (chunks) before processing them with Dask. Each batch will contain multiple files that a single task will process.\n",
    "- Modify the Dask Task: Change the Dask delayed function to accept a list of HTML files instead of a single file and modify the parsing function accordingly if necessary.\n",
    "- Process Batches in Parallel: Use Dask to process each batch of files in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiofiles\n",
    "\n",
    "async def read_html_files_async(directory_path):\n",
    "    html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "    return html_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_floor(details):\n",
    "    if len(details) >= 3 and 'andar' in details[2]:\n",
    "        return details[2]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df):\n",
    "    df['home_type'] = df['title'].apply(lambda x: x.split()[0] if x else '')\n",
    "    df['home_size'] = df['additional_details'].apply(lambda x: x[0] if len(x) >= 1 else None)\n",
    "    df['home_area'] = df['additional_details'].apply(lambda x: int(re.search(r'\\d+', x[1]).group()) if len(x) >= 2 else None)\n",
    "    df['floor'] = df['additional_details'].apply(extract_floor)\n",
    "\n",
    "    df['elevator'] = df['floor'].apply(lambda x: 'com elevador' in str(x))\n",
    "    df['floor'] = df['floor'].apply(lambda x: int(re.search(r'\\d+', str(x)).group()) if x and re.search(r'\\d+', str(x)) else 0)\n",
    "\n",
    "    df['price_per_sqr_meter'] = df['price'] / df['home_area']\n",
    "    df.drop(columns=['additional_details'], inplace=True)\n",
    "    df.fillna({'elevator': False, 'floor': 0}, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(article):\n",
    "    price_info = article.find('span', class_='item-price')\n",
    "    if price_info:\n",
    "        numeric_price = re.findall(r'\\d+', price_info.text.replace('.', ''))\n",
    "        return int(numeric_price[0]) if numeric_price else None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_listing_info(article):\n",
    "    listing_info = {\n",
    "        'title': (title_link := article.find('a', class_='item-link')) and title_link.get('title', ''),\n",
    "        'link': 'https://www.idealista.pt' + (title_link.get('href', '') if title_link else ''),\n",
    "        'garage': 'Yes' if article.find('span', class_='item-parking') else 'No',\n",
    "        'price': extract_price(article),\n",
    "        'description': article.find('div', class_=\"item-description description\").text.strip(),\n",
    "        'additional_details': [detail.get_text(strip=True) for detail in article.find_all('span', class_='item-detail')]\n",
    "    }\n",
    "    return listing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_to_dataframe(file_path):\n",
    "    # Read the HTML file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    listings = []\n",
    "    articles = soup.find_all('article', class_=['item', 'extended-item'])\n",
    "    for article in articles:\n",
    "        listing_info = extract_listing_info(article)\n",
    "        listings.append(listing_info)\n",
    "\n",
    "    df = pd.DataFrame(listings)\n",
    "    if not df.empty:\n",
    "        df = process_dataframe(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_files(file_list, chunk_size):\n",
    "    # Generator that yields chunks of files, more efficient that list comprehension\n",
    "    for i in range(0, len(file_list), chunk_size):\n",
    "        yield file_list[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_files(parsing_function, files):\n",
    "    # This function processes a batch of HTML files and returns a DataFrame\n",
    "    dataframes = []\n",
    "    for file in files:\n",
    "        df = parsing_function(file)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(parsing_function, directory_path=\"idealista\", chunk_size=20):\n",
    "    db = DatabaseConnection('house_prices.db')\n",
    "    with db.managed_cursor() as con:\n",
    "        # List HTML files in the directory\n",
    "        html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "\n",
    "        # Create chunks of HTML files\n",
    "        file_chunks = list(chunk_files(html_files, chunk_size))\n",
    "\n",
    "        # Use Dask to parse HTML files in batches\n",
    "        delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
    "        with ProgressBar():\n",
    "            computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
    "\n",
    "        # Filter out None results from batches\n",
    "        filtered_dataframes = [df for df in computed_dataframes if df is not None]\n",
    "\n",
    "        # Concatenate dataframes efficiently\n",
    "        if filtered_dataframes:\n",
    "            final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "            # Insert into database\n",
    "            con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "            final_df.to_sql(directory_path, con, index=False, if_exists='replace', method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 16.27 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1981086236.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  final_df.to_sql(directory_path, con, index=False, if_exists='replace')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 228.601 s\n",
      "File: C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1981086236.py\n",
      "Function: process_data at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def process_data(parsing_function, directory_path=\"idealista\", chunk_size=10):\n",
      "     2         1         50.0     50.0      0.0      db = DatabaseConnection('house_prices.db')\n",
      "     3         2    2785587.0    1e+06      0.1      with db.managed_cursor() as con:\n",
      "     4                                                   # List HTML files in the directory\n",
      "     5         1      47898.0  47898.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
      "     6                                           \n",
      "     7                                                   # Create chunks of HTML files\n",
      "     8         1        368.0    368.0      0.0          file_chunks = list(chunk_files(html_files, chunk_size))\n",
      "     9                                           \n",
      "    10                                                   # Use Dask to parse HTML files in batches\n",
      "    11         1      68009.0  68009.0      0.0          delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
      "    12         2        386.0    193.0      0.0          with ProgressBar():\n",
      "    13         1  164562665.0    2e+08      7.2              computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
      "    14                                           \n",
      "    15                                                   # Filter out None results from batches\n",
      "    16         1         86.0     86.0      0.0          filtered_dataframes = [df for df in computed_dataframes if df is not None]\n",
      "    17                                           \n",
      "    18                                                   # Concatenate dataframes efficiently\n",
      "    19         1          3.0      3.0      0.0          if filtered_dataframes:\n",
      "    20         1      69317.0  69317.0      0.0              final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
      "    21                                           \n",
      "    22                                                       # Insert into database\n",
      "    23         1     328203.0 328203.0      0.0              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
      "    24         1 2118142445.0    2e+09     92.7              final_df.to_sql(directory_path, con, index=False, if_exists='replace')"
     ]
    }
   ],
   "source": [
    "%lprun -f process_data process_data(parse_html_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Timer unit: 1e-07 s\n",
    "\n",
    "Total time: 228.601 s\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def process_data(parsing_function, directory_path=\"idealista\", chunk_size=10):\n",
    "     2         1         50.0     50.0      0.0      db = DatabaseConnection('house_prices.db')\n",
    "     3         2    2785587.0    1e+06      0.1      with db.managed_cursor() as con:\n",
    "     4                                                   # List HTML files in the directory\n",
    "     5         1      47898.0  47898.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "     6                                           \n",
    "     7                                                   # Create chunks of HTML files\n",
    "     8         1        368.0    368.0      0.0          file_chunks = list(chunk_files(html_files, chunk_size))\n",
    "     9                                           \n",
    "    10                                                   # Use Dask to parse HTML files in batches\n",
    "    11         1      68009.0  68009.0      0.0          delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
    "    12         2        386.0    193.0      0.0          with ProgressBar():\n",
    "    13         1  164562665.0    2e+08      7.2              computed_dataframes = dask.compute(*delayed_dataframes, scheduler='processes')\n",
    "    14                                           \n",
    "    15                                                   # Filter out None results from batches\n",
    "    16         1         86.0     86.0      0.0          filtered_dataframes = [df for df in computed_dataframes if df is not None]\n",
    "    17                                           \n",
    "...\n",
    "    20         1      69317.0  69317.0      0.0              final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "    21                                           \n",
    "    22                                                       # Insert into database\n",
    "    23         1     328203.0 328203.0      0.0              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "    24         1 2118142445.0    2e+09     92.7              final_df.to_sql(directory_path, con, index=False, if_exists='replace')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Dask and lxml for parsing](#toc0_)\n",
    "\n",
    "  \n",
    "```{mermaid}\n",
    "graph LR\n",
    "    A[Raw HTML Files] --> B[Python Script<br>with lxml & Dask]\n",
    "    B --> C[Extract Data]\n",
    "    C --> D[Transform to Structured Format]\n",
    "    D --> E[Save to Parquet]\n",
    "\n",
    "    subgraph Process\n",
    "        B -->|Batched HTML Files| F[Read HTML Files]\n",
    "        F --> G[Parallel Processing<br>with Dask]\n",
    "        G --> H[Merge DataFrames]\n",
    "        H --> I[Structured Data]\n",
    "    end\n",
    "\n",
    "    A --> F\n",
    "    G -->|Processed Data| H\n",
    "    H -->|DataFrames| I\n",
    "    I --> E\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "\n",
    "def parse_html_files_to_dataframe(file_paths):\n",
    "    listings = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Read and parse each HTML file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            parsed_html = html.parse(file)\n",
    "\n",
    "        # Extract articles that represent real estate listings\n",
    "        articles = parsed_html.xpath('//article[contains(@class, \"item\") or contains(@class, \"extended-item\")]')\n",
    "        for article in articles:\n",
    "            listing_info = extract_listing_info(article)\n",
    "            listings.append(listing_info)\n",
    "\n",
    "    # Create a DataFrame from all collected listings\n",
    "    df = pd.DataFrame(listings)\n",
    "    if not df.empty:\n",
    "        df = process_dataframe(df)\n",
    "    return df\n",
    "\n",
    "def extract_listing_info(article):\n",
    "    # Extracts data from an article and returns a dictionary of listing info\n",
    "    return {\n",
    "        'title': article.xpath('.//a[@class=\"item-link\"]/@title')[0] if article.xpath('.//a[@class=\"item-link\"]/@title') else '',\n",
    "        'link': 'https://www.idealista.pt' + (article.xpath('.//a[@class=\"item-link\"]/@href')[0] if article.xpath('.//a[@class=\"item-link\"]/@href') else ''),\n",
    "        'description': article.xpath('.//div[@class=\"item-description description\"]/text()')[0].strip() if article.xpath('.//div[@class=\"item-description description\"]/text()') else '',\n",
    "        'garage': \"Yes\" if article.xpath('.//span[@class=\"item-parking\"]') else \"No\",\n",
    "        'price': extract_price(article.xpath('.//span[@class=\"item-price\"]/text()')),\n",
    "        'additional_details': article.xpath('.//span[@class=\"item-detail\"]/text()')\n",
    "    }\n",
    "\n",
    "def extract_price(price_texts):\n",
    "    if price_texts:\n",
    "        numeric_price = re.findall(r'\\d+', price_texts[0].replace('.', ''))\n",
    "        return int(numeric_price[0]) if numeric_price else None\n",
    "    return None\n",
    "\n",
    "def process_dataframe(df):\n",
    "    df['home_size'] = df['additional_details'].apply(lambda x: x[0] if x else None)\n",
    "    df['home_area'] = df['additional_details'].apply(lambda x: int(re.search(r'\\d+', x[1]).group()) if len(x) >= 2 else None)\n",
    "    df['floor'] = df['additional_details'].apply(lambda x: x[2] if len(x) >= 3 and 'andar' in x[2] else None)\n",
    "    df['elevator'] = df['floor'].apply(lambda x: 'com elevador' in str(x) if x else False)\n",
    "    df['floor'] = df['floor'].apply(lambda x: int(re.search(r'\\d+', str(x)).group()) if x else 0)\n",
    "    df['price_per_sqr_meter'] = df['price'] / df['home_area']\n",
    "    df.drop(columns=['additional_details'], inplace=True)\n",
    "    df.fillna({'elevator': False, 'floor': 0}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(parsing_function, directory_path=\"idealista\", chunk_size=10):\n",
    "    db = DatabaseConnection('house_prices.db')\n",
    "    with db.managed_cursor() as con:\n",
    "        # List HTML files in the directory\n",
    "        html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "\n",
    "        # Create chunks of HTML files\n",
    "        file_chunks = list(chunk_files(html_files, chunk_size))\n",
    "\n",
    "        # Use Dask to parse HTML files in batches\n",
    "        delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
    "        with ProgressBar():\n",
    "            computed_dataframes = dask.compute(*delayed_dataframes, \n",
    "                                               scheduler='processes')\n",
    "\n",
    "        # Filter out None results from batches\n",
    "        filtered_dataframes = [df for df in computed_dataframes if df is not None]\n",
    "\n",
    "        # Concatenate dataframes efficiently\n",
    "        if filtered_dataframes:\n",
    "            final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "\n",
    "            # Insert into database\n",
    "            con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "            final_df.to_sql(directory_path, con, index=False, if_exists='replace', method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_files(parsing_function, files):\n",
    "    # This function processes a batch of HTML files and returns a DataFrame\n",
    "    dataframes = []\n",
    "    for file in files:\n",
    "        df = parsing_function(file)\n",
    "        if df is not None and not df.empty:\n",
    "            dataframes.append(df)\n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "graph TD;\n",
    "    A[parse_html_files_to_df<br>Input: list of strings<br>Output: DataFrame] --> C[extract_listing_info<br>Input: lxml html Element<br>Output: dictionary];\n",
    "    C --> D[extract_price<br>Input: list of strings<br>Output: Optional int];\n",
    "    D --> E[Database Insertion<br>Input: DataFrame<br>Output: None];\n",
    "    A --> B[process_dataframe<br>Input: DataFrame<br>Output: DataFrame];\n",
    "    B --> E;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 277\n",
      "Chunk size: 10\n",
      "Number of chunks: 28\n",
      "Chunk 1: 10 files\n",
      "Chunk 2: 10 files\n",
      "Chunk 3: 10 files\n",
      "Chunk 4: 10 files\n",
      "Chunk 5: 10 files\n",
      "Chunk 6: 10 files\n",
      "Chunk 7: 10 files\n",
      "Chunk 8: 10 files\n",
      "Chunk 9: 10 files\n",
      "Chunk 10: 10 files\n",
      "Chunk 11: 10 files\n",
      "Chunk 12: 10 files\n",
      "Chunk 13: 10 files\n",
      "Chunk 14: 10 files\n",
      "Chunk 15: 10 files\n",
      "Chunk 16: 10 files\n",
      "Chunk 17: 10 files\n",
      "Chunk 18: 10 files\n",
      "Chunk 19: 10 files\n",
      "Chunk 20: 10 files\n",
      "Chunk 21: 10 files\n",
      "Chunk 22: 10 files\n",
      "Chunk 23: 10 files\n",
      "Chunk 24: 10 files\n",
      "Chunk 25: 10 files\n",
      "Chunk 26: 10 files\n",
      "Chunk 27: 10 files\n",
      "Chunk 28: 7 files\n",
      "[                                        ] | 0% Completed | 587.00 us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 19.93 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1320541976.py:30: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  final_df.to_sql(directory_path, con, index=False, if_exists='replace', method='multi')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 22.1248 s\n",
      "File: C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_16744\\1320541976.py\n",
      "Function: process_data at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def process_data(parsing_function, directory_path=\"idealista\", chunk_size=10):\n",
      "     2         1         41.0     41.0      0.0      db = DatabaseConnection('house_prices.db')\n",
      "     3         2    2080322.0    1e+06      0.9      with db.managed_cursor() as con:\n",
      "     4                                                   # List HTML files in the directory\n",
      "     5         1      76743.0  76743.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
      "     6                                           \n",
      "     7                                                   # Create chunks of HTML files\n",
      "     8         1        662.0    662.0      0.0          file_chunks = list(chunk_files(html_files, chunk_size))\n",
      "     9         1       3587.0   3587.0      0.0          print(f\"Total files: {len(html_files)}\")\n",
      "    10         1        160.0    160.0      0.0          print(f\"Chunk size: {chunk_size}\")\n",
      "    11         1        139.0    139.0      0.0          print(f\"Number of chunks: {len(file_chunks)}\")\n",
      "    12        29        198.0      6.8      0.0          for idx, chunk in enumerate(file_chunks):\n",
      "    13        28       6534.0    233.4      0.0              print(f\"Chunk {idx + 1}: {len(chunk)} files\")\n",
      "    14                                           \n",
      "    15                                                   # Use Dask to parse HTML files in batches\n",
      "    16         1      97062.0  97062.0      0.0          delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
      "    17         2        521.0    260.5      0.0          with ProgressBar():\n",
      "    18         2  201413020.0    1e+08     91.0              computed_dataframes = dask.compute(*delayed_dataframes, \n",
      "    19         1          5.0      5.0      0.0                                                 scheduler='processes')\n",
      "    20                                           \n",
      "    21                                                   # Filter out None results from batches\n",
      "    22         1        101.0    101.0      0.0          filtered_dataframes = [df for df in computed_dataframes if df is not None]\n",
      "    23                                           \n",
      "    24                                                   # Concatenate dataframes efficiently\n",
      "    25         1          3.0      3.0      0.0          if filtered_dataframes:\n",
      "    26         1      81922.0  81922.0      0.0              final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
      "    27                                           \n",
      "    28                                                       # Insert into database\n",
      "    29         1     458093.0 458093.0      0.2              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
      "    30         1   17028626.0    2e+07      7.7              final_df.to_sql(directory_path, con, index=False, if_exists='replace', method='multi')"
     ]
    }
   ],
   "source": [
    "%lprun -f process_data process_data(parse_html_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Timer unit: 1e-07 s\n",
    "\n",
    "Total time: 22.1248 s\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def process_data(parsing_function, directory_path=\"idealista\", chunk_size=10):\n",
    "     2         1         41.0     41.0      0.0      db = DatabaseConnection('house_prices.db')\n",
    "     3         2    2080322.0    1e+06      0.9      with db.managed_cursor() as con:\n",
    "     4                                                   # List HTML files in the directory\n",
    "     5         1      76743.0  76743.0      0.0          html_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.html')]\n",
    "     6                                           \n",
    "     7                                                   # Create chunks of HTML files\n",
    "     8         1        662.0    662.0      0.0          file_chunks = list(chunk_files(html_files, chunk_size))\n",
    "     9         1       3587.0   3587.0      0.0          print(f\"Total files: {len(html_files)}\")\n",
    "    10         1        160.0    160.0      0.0          print(f\"Chunk size: {chunk_size}\")\n",
    "    11         1        139.0    139.0      0.0          print(f\"Number of chunks: {len(file_chunks)}\")\n",
    "    12        29        198.0      6.8      0.0          for idx, chunk in enumerate(file_chunks):\n",
    "    13        28       6534.0    233.4      0.0              print(f\"Chunk {idx + 1}: {len(chunk)} files\")\n",
    "    14                                           \n",
    "    15                                                   # Use Dask to parse HTML files in batches\n",
    "    16         1      97062.0  97062.0      0.0          delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
    "    17         2        521.0    260.5      0.0          with ProgressBar():\n",
    "...\n",
    "    26         1      81922.0  81922.0      0.0              final_df = pd.concat(filtered_dataframes, axis=0, ignore_index=True)\n",
    "    27                                           \n",
    "    28                                                       # Insert into database\n",
    "    29         1     458093.0 458093.0      0.2              con.execute(f\"DROP TABLE IF EXISTS {directory_path}\")\n",
    "    30         1   17028626.0    2e+07      7.7              final_df.to_sql(directory_path, con, index=False, if_exists='replace', method='multi')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Analysis of the results](#toc0_)\n",
    "\n",
    "**Over 20 times time reduction from baseline!**\n",
    "\n",
    "```\n",
    "| Library Combination         | Time (s)  |\n",
    "|-----------------------------|-----------|\n",
    "| Pandas + BS4                | 478.326   |\n",
    "| Dask + BS4                  | 257.473   |\n",
    "| Dask + BS4/chunking files   | 228.601   |\n",
    "| Dask + lxml                 | 22.1248   |\n",
    "```\n",
    "\n",
    "#### <a id='toc1_4_1_1_'></a>[Lxml](#toc0_)\n",
    "As we can see the biggest performance comes from implementing lxml instead of BS4. Lxml provides an API to parse DOM data using tree iterators which can handle complex data structures with the lightning speed of C. This matters a lot, specially in the age of GenAI since there's a lot of information from several sources to parse and this efficiency is for sure a way to scale and decrease costs.\n",
    "\n",
    "#### <a id='toc1_4_1_2_'></a>[Dask](#toc0_)\n",
    "\n",
    "Its worth noting the performance increase from Pandas to Dask. It could be even better if we fixed the issue with unmatching dataframes during concatenation or provided a dask dataframe schema. We could also explore the optimal batch for chunk processing as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Conclusions](#toc0_)\n",
    "\n",
    "1. Build a basic parser with BS4 and pandas\n",
    "2. Test the parser, make sure it returns the data in the right formats\n",
    "3. Test the data types, if possible **optimize data types**\n",
    "4. User **function profiling tools** to assess bottlenecks\n",
    "5. If the parser is complex, use **functional decomposition** to make it simpler\n",
    "6. Apply **vectorization** instead of loops (loop over columns instead of mixed data types or python loops) (ask GPT to help you)\n",
    "7. Use dasks **lazy evaluation** and instantiation for ops\n",
    "8. Replace BS4 by lxml (ask GPT to help you). lxml is based on C and is lightning fast.\n",
    "   1. This is where you might want to add efficiency at the cost of complexity\n",
    "9.  Parse html files in chunks of files if they are small. First chunk into list of list then feed to parsing function. Parsing function should take multiple file paths.\n",
    "    1.  You can use **async** processing if need be.\n",
    "\n",
    "file_chunks = list(chunk_files(html_files, chunk_size))\n",
    "delayed_dataframes = [dask.delayed(process_html_files)(parsing_function, chunk) for chunk in file_chunks]\n",
    "1.  Pass custom metadata to dask before computing, with optimized data types\n",
    "2.  Use chunk insert to SQL, or 'multi'/asynchronous connection\n",
    "\n",
    "## <a id='toc1_6_'></a>[Considerations during optimization](#toc0_)\n",
    "\n",
    "- Reducing I/O operations reading files in parallelized chunks\n",
    "- Trace the most important functions and avoid waste (lean thinking)\n",
    "- Improving the efficiency of data processing by streamlining functions (reducing overhead)\n",
    "- Minimize memory usage by using lazy evaluation and optimized data types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}